import os
import torch
import torch.nn as nn
from torchvision import transforms
from torchvision.models import vit_b_16, ViT_B_16_Weights
from PIL import Image
import numpy as np

from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import HumanMessage, SystemMessage
from dotenv import load_dotenv

# ================== PH·∫¶N 1: LOAD MODEL VI·ªÜT ==================
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Load model ƒë√£ train
checkpoint = torch.load("./checkpoints/best.pt", map_location=device)
weights = ViT_B_16_Weights.IMAGENET1K_V1
model = vit_b_16(weights=weights)
num_classes = len(os.listdir(r"C:\Users\tam\Documents\data\PlantVillage_Split\test"))
model.heads.head = nn.Linear(model.heads.head.in_features, num_classes)
model.load_state_dict(checkpoint["model_state_dict"])
model.to(device)
model.eval()

transform = weights.transforms()
class_names = os.listdir(r"C:\Users\tam\Documents\data\PlantVillage_Split\test")

def predict_disease(image_path):
    img = Image.open(image_path).convert("RGB")
    img_t = transform(img).unsqueeze(0).to(device)
    with torch.no_grad():
        outputs = model(img_t)
        probs = torch.softmax(outputs, dim=1).cpu().numpy()[0]
        pred_idx = np.argmax(probs)
    return class_names[pred_idx], probs[pred_idx]


# ================== PH·∫¶N 2: T·∫†O CHATBOT N√îNG NGHI·ªÜP ==================
load_dotenv()

# Load t√†i li·ªáu n√¥ng nghi·ªáp (PDF s√°ch, t√†i li·ªáu)
loader = PyPDFLoader("books/9cc1f47a-en.pdf")   # thay b·∫±ng s√°ch th·∫≠t
documents = loader.load()

splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
chunks = splitter.split_documents(documents)
combined_text = "\n\n".join([chunk.page_content for chunk in chunks[:20]])  # l·∫•y 20 chunk ƒë·∫ßu

llm = ChatGoogleGenerativeAI(
    model="gemini-2.0-flash",
    temperature=0.3,
    google_api_key=os.getenv("GOOGLE_API_KEY")
)


def chatbot(query, context_info=""):
    messages = [
        SystemMessage(content="B·∫°n l√† tr·ª£ l√Ω AI chuy√™n v·ªÅ n√¥ng nghi·ªáp. Tr·∫£ l·ªùi th√¢n thi·ªán, d·ªÖ hi·ªÉu cho n√¥ng d√¢n."),
        HumanMessage(content=f"T√†i li·ªáu:\n{combined_text}\n\nTh√¥ng tin b·ªánh c√¢y: {context_info}\n\nC√¢u h·ªèi: {query}")
    ]
    response = llm.invoke(messages)
    return response.content


# ================== PH·∫¶N 3: DEMO ==================
if __name__ == "__main__":
    print("üåø Nh·∫≠p ƒë∆∞·ªùng d·∫´n ·∫£nh l√° c√¢y (ho·∫∑c g√µ 'skip' ƒë·ªÉ b·ªè qua nh·∫≠n di·ªán).")
    img_path = input("·∫¢nh l√° c√¢y: ")

    disease_info = ""
    if img_path.lower() != "skip":
        disease, confidence = predict_disease(img_path)
        disease_info = f"H·ªá th·ªëng d·ª± ƒëo√°n l√° c√¢y b·ªã: {disease} (ƒë·ªô tin c·∫≠y {confidence:.2f})"
        print("üîç", disease_info)

    print("\nü§ñ Chatbot n√¥ng nghi·ªáp s·∫µn s√†ng! G√µ 'exit' ƒë·ªÉ tho√°t.")
    while True:
        query = input("üë§ B·∫°n: ")
        if query.lower() in ["exit", "quit"]:
            break

        answer = chatbot(query, context_info=disease_info)
        print("\nü§ñ Bot:", answer)
